{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloom Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "class BloomFilter:\n",
    "    def __init__(self,size,n_hashes):\n",
    "        self.size = size\n",
    "        self.n_hashes = n_hashes\n",
    "        self.bit_array = [0] * size\n",
    "    \n",
    "    def hash_func(self,key,seed):\n",
    "        return int(hashlib.sha256((str(key)+str(seed)).encode('utf-8')).hexdigest(),16)%self.size\n",
    "\n",
    "    def add(self,key):\n",
    "        for i in range(self.n_hashes):\n",
    "            hash_value = self.hash_func(key,i)\n",
    "            self.bit_array[hash_value] = 1\n",
    "    \n",
    "    def __contains__(self,key):\n",
    "        for i in range(self.n_hashes):\n",
    "            hash_value = self.hash_func(key,i)\n",
    "            if self.bit_array[hash_value] == 0:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "bf = BloomFilter(100,3)\n",
    "\n",
    "bf.add('hello')\n",
    "bf.add('world')\n",
    "\n",
    "print('hello' in bf) #true\n",
    "print('earth' in bf) #false\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(2, 3), (2, 4), (1, 2), (3, 4)}\n"
     ]
    }
   ],
   "source": [
    "# def apriori(data,threshold):\n",
    "\n",
    "#     cand_items = set()\n",
    "#     for i in range(len(data[0])):\n",
    "#         for j in range(i + 1, len(data[0])):\n",
    "#             cand_items.add((data[0][i], data[0][j]))\n",
    "\n",
    "#     freq_items = set()\n",
    "#     for cand_item in cand_items:\n",
    "#         support = 0\n",
    "#         for trans in data:\n",
    "#             if all(item in trans for item in cand_items):\n",
    "#                 support += 1\n",
    "#         if support >= threshold:\n",
    "#             freq_items.add(cand_item)\n",
    "#     return freq_items\n",
    "\n",
    "# data = [\n",
    "#     [1,2,3,4],\n",
    "#     [1,2,4],\n",
    "#     [1,2],\n",
    "#     [2,3,4],\n",
    "#     [2,3],\n",
    "#     [3,4],\n",
    "#     [2,4]\n",
    "# ]\n",
    "# threshold = 3\n",
    "# result = apriori(data,threshold)\n",
    "# print(result)\n",
    "\n",
    "\n",
    "def apriori(dataset, min_support):\n",
    "  \"\"\"\n",
    "  Finds all frequent itemsets in the dataset that meet the minimum support threshold.\n",
    "\n",
    "  Args:\n",
    "    dataset: A list of transactions. Each transaction is a list of items.\n",
    "    min_support: The minimum number of times an itemset must appear in the dataset to be considered frequent.\n",
    "\n",
    "  Returns:\n",
    "    A set of frequent itemsets.\n",
    "  \"\"\"\n",
    "  # Generate a set of candidate itemsets.\n",
    "  candidate_itemsets = set()\n",
    "  for i in range(len(dataset[0])):\n",
    "    for j in range(i + 1, len(dataset[0])):\n",
    "      candidate_itemsets.add((dataset[0][i], dataset[0][j]))\n",
    "  # Check each candidate itemset to see if it meets the minimum support threshold.\n",
    "  frequent_itemsets = set()\n",
    "  for candidate_itemset in candidate_itemsets:\n",
    "    support = 0\n",
    "    for transaction in dataset:\n",
    "      if all(item in transaction for item in candidate_itemset):\n",
    "        support += 1\n",
    "    if support >= min_support:\n",
    "      frequent_itemsets.add(candidate_itemset)\n",
    "  return frequent_itemsets\n",
    "# Example usage\n",
    "transactions = [\n",
    "    [1, 2, 3, 4],\n",
    "    [1, 2, 4],\n",
    "    [1, 2],\n",
    "    [2, 3, 4],\n",
    "    [2, 3],\n",
    "    [3, 4],\n",
    "    [2, 4]\n",
    "]\n",
    "support_threshold =3\n",
    "print(apriori(transactions, support_threshold))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCY algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(2, 3), (2, 4), (1, 2), (3, 4)}\n"
     ]
    }
   ],
   "source": [
    "def pcy(dataset, min_support):\n",
    "  \"\"\"\n",
    "  Finds all frequent itemsets in the dataset that meet the minimum support threshold.\n",
    "\n",
    "  Args:\n",
    "    dataset: A list of transactions. Each transaction is a list of items.\n",
    "    min_support: The minimum number of times an itemset must appear in the dataset to be considered frequent.\n",
    "\n",
    "  Returns:\n",
    "    A set of frequent itemsets.\n",
    "  \"\"\"\n",
    "  # Create a hash table of all possible itemsets.\n",
    "  itemsets = set()\n",
    "  for i in range(len(dataset[0])):\n",
    "    for j in range(i + 1, len(dataset[0])):\n",
    "      itemsets.add((dataset[0][i], dataset[0][j]))\n",
    "  # Scan the dataset once, counting the number of times each itemset appears.\n",
    "  counts = {}\n",
    "  for transaction in dataset:\n",
    "    for itemset in itemsets:\n",
    "      if all(item in transaction for item in itemset):\n",
    "        counts[itemset] = counts.get(itemset, 0) + 1\n",
    "  # Use a pruning algorithm to remove all itemsets that do not meet the minimum support threshold.\n",
    "  frequent_itemsets = set()\n",
    "  for itemset, count in counts.items():\n",
    "    if count >= min_support:\n",
    "      frequent_itemsets.add(itemset)\n",
    "  return frequent_itemsets\n",
    "# Example usage\n",
    "transactions = [[1, 2, 3, 4],[1, 2, 4],\n",
    "    [1, 2],\n",
    "    [2, 3, 4],\n",
    "    [2, 3],\n",
    "    [3, 4],\n",
    "    [2, 4]]\n",
    "support_threshold = 3\n",
    "print(pcy(transactions, support_threshold))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Lorem', 2), ('Ipsum', 2), ('is', 1), ('simply', 1), ('dummy', 2), ('text', 2), ('of', 2), ('the', 4), ('printing', 1), ('and', 2), ('typesetting', 1), ('industry.', 1), ('has', 2), ('been', 1), (\"industry's\", 1), ('standard', 1), ('ever', 1), ('since', 1), ('1500s,', 1), ('when', 1), ('an', 1), ('unknown', 1), ('printer', 1), ('took', 1), ('a', 2), ('galley', 1), ('type', 2), ('scrambled', 1), ('it', 1), ('to', 1), ('make', 1), ('specimen', 1), ('book.', 1), ('It', 1), ('survived', 1), ('not', 1), ('only', 1), ('five', 1), ('centuries,', 1), ('but', 1), ('also', 1), ('leap', 1), ('into', 1), ('electronic', 1), ('typesetting,', 1), ('remaining', 1), ('essentially', 1), ('unchanged.', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def mapper(text):\n",
    "    words = text.split()\n",
    "    key_value_pairs = [(word, 1) for word in words]\n",
    "    return key_value_pairs\n",
    "\n",
    "def reducer(key, values):\n",
    "    word_count = sum(values)\n",
    "    return (key, word_count)\n",
    "\n",
    "def map_reduce(data, mapper, reducer):\n",
    "    # Map phase\n",
    "    intermediate_results = []\n",
    "    for item in data:\n",
    "        intermediate_results.extend(mapper(item))\n",
    "\n",
    "    # Reduce phase\n",
    "    grouped_items = {}\n",
    "    for key, value in intermediate_results:\n",
    "        if key not in grouped_items:\n",
    "            grouped_items[key] = []\n",
    "        grouped_items[key].append(value)\n",
    "\n",
    "    result = []\n",
    "    for key, values in grouped_items.items():\n",
    "        result.append(reducer(key, values))\n",
    "\n",
    "    return result\n",
    "\n",
    "sentences = [\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.\"]\n",
    "result = map_reduce(sentences, mapper, reducer)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This: 2\n",
      "is: 1\n",
      "a: 1\n",
      "sample: 1\n",
      "text.: 1\n",
      "text: 1\n",
      "can: 1\n",
      "be: 1\n",
      "used: 1\n",
      "for: 1\n",
      "word: 1\n",
      "count.: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def mapper(text):\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Emit key-value pairs of (word, 1)\n",
    "    return [(word, 1) for word in words]\n",
    "\n",
    "def reducer(pairs):\n",
    "    word_counts = defaultdict(int)\n",
    "\n",
    "    # Aggregate word counts\n",
    "    for word, count in pairs:\n",
    "        word_counts[word] += count\n",
    "\n",
    "    # Emit final word counts\n",
    "    return word_counts.items()\n",
    "\n",
    "# Example usage\n",
    "text = \"This is a sample text. This text can be used for word count.\"\n",
    "\n",
    "# Map phase\n",
    "mapped_pairs = mapper(text)\n",
    "\n",
    "# Shuffle and sort (optional in this case)\n",
    "\n",
    "# Reduce phase\n",
    "reduced_pairs = reducer(mapped_pairs)\n",
    "\n",
    "# Print word counts\n",
    "for word, count in reduced_pairs:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This: 2\n",
      "a: 1\n",
      "be: 1\n",
      "can: 1\n",
      "count.: 1\n",
      "for: 1\n",
      "is: 1\n",
      "sample: 1\n",
      "text: 1\n",
      "text.: 1\n",
      "used: 1\n",
      "word: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def mapper(text):\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Emit key-value pairs of (word, 1)\n",
    "    return [(word, 1) for word in words]\n",
    "\n",
    "def reducer(pairs):\n",
    "    word_counts = defaultdict(int)\n",
    "\n",
    "    # Aggregate word counts\n",
    "    for word, count in pairs:\n",
    "        word_counts[word] += count\n",
    "\n",
    "    # Emit final word counts\n",
    "    return word_counts.items()\n",
    "\n",
    "# Example usage\n",
    "text = \"This is a sample text. This text can be used for word count.\"\n",
    "\n",
    "# Map phase\n",
    "mapped_pairs = mapper(text)\n",
    "\n",
    "# Shuffle and sort (optional in this case)\n",
    "sorted_pairs = sorted(mapped_pairs, key=lambda x: x[0])\n",
    "# Reduce phase\n",
    "reduced_pairs = reducer(sorted_pairs)\n",
    "\n",
    "# Print word counts\n",
    "for word, count in reduced_pairs:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop commands"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hdfs namenode -format\n",
    "hdfs dfs -mkdir /user\n",
    "hdfs dfs -ls\n",
    "hdfs dfs -copyFromLocal C:/Data/Exp1.txt /user\n",
    "hdfs dfs -cat /user/filename.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
